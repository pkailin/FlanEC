training:
  checkpoint_dir: "t5-models/"
  epochs: 10
  batch_size: 4
  warmup_ratio: 0.1
  weight_decay: 0.01
  log_dir: "t5-logs"
  logging_steps: 1000
  eval_strategy: "steps" #epoch
  save_strategy: "steps" #epoch
  eval_steps: 6025 #2000
  save_steps: 6025 #2000
  load_best_model_at_end: true
  learning_rate: 0.00005
  dataloader_num_workers: 4
  save_total_limit: null #3
  use_cuda: true
  fp16: true #false
  metric_for_best_model: "wer"
  greater_is_better: false
  hub_model_id: "leave_empty"
  push_to_hub: false
  gradient_accumulation_steps: 1

model:
  model_tag: "google-t5/t5-base"

data:
  train_file: "nbest_train_dev.json"
  test_file: "nbest_test.json"
  dataset_file: null
  max_input_length: 1280
  max_output_length: 256
  train_val_split: 0.9 #0.99
  truncation: true
  prefix_prompt: "Generate the correct transcription for the following n-best list of ASR hypotheses:"
  suffix_prompt: ""
  use_source: false

inference:
  specific_test_file: ""
  specific_checkpoint_dir: "morenolq/flanec-base-cd"
